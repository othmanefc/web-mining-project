{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55cde399-94a4-46c6-b7a7-74e7c895c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from skopt.space import Real, Integer\n",
    "from scipy import sparse as ssp\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utils import load_data\n",
    "from modelling import plot_estimators, tune_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ac3d31-272a-41c9-aadd-1229b3ffdb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = 'comments_processed.ft'\n",
    "\n",
    "train_df, test_df = load_data(PATH_DATA, split=True, is_csv=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c594abb-1bd4-45f2-8bc5-1a5978d1410a",
   "metadata": {},
   "source": [
    "Let's split columns through types and apply a transformation to each, with encoding for categorical variables and scaling for numerical ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5eed2fd-a001-493d-9dd5-580b6f18238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['subreddit', 'ups', \n",
    "             'link_id', 'id', 'name', 'body', 'author', 'parent_id',\n",
    "             'subreddit_id']\n",
    "categorical_cols = ['is_root', 'weekday', 'deleted', 'author_is_influential', \n",
    "                    'contains_url', 'contains_html', 'author_is_moderator', 'clusters']\n",
    "\n",
    "numeric_cols = [col for col in train_df.select_dtypes(include=np.number).columns\n",
    "                if col not in drop_cols and col not in categorical_cols]\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('encoder', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric_variables', numerical_transformer, numeric_cols),\n",
    "        ('categorical_variables', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c537c2d9-858e-443c-aa8a-857c9f8e85f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting...\n",
      "[Pipeline] ......... (step 1 of 2) Processing data_prep, total=   4.4s\n",
      "[Pipeline] .............. (step 2 of 2) Processing lgbm, total=   4.0s\n",
      "fitting...\n",
      "[Pipeline] ......... (step 1 of 2) Processing data_prep, total=   4.5s\n",
      "[Pipeline] ............... (step 2 of 2) Processing xgb, total= 2.5min\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('lgbm', lgbm.LGBMRegressor()),\n",
    "    ('xgb', xgb.XGBRegressor())\n",
    "]\n",
    "pipe_all = Pipeline(steps=[('col_transfo', preprocessor)])\n",
    "pipes = {}\n",
    "for model in estimators:\n",
    "    print('fitting...')\n",
    "    pipe = Pipeline(steps=[('data_prep', pipe_all), model], verbose=True)\n",
    "    pipe.fit(train_df.drop(columns='ups'), train_df.ups)\n",
    "    pipes[pipe.steps[1][0]] = pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e28de9-e0ef-4df5-b51c-f2966743086c",
   "metadata": {},
   "source": [
    "Let's plot cross_validation results to see how models compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11c6db68-32da-480f-8a70-5a1be045adbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV on lgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   55.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV on xgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 13.4min finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_metrics={'mae': make_scorer(mean_absolute_error)}\n",
    "estimator_names=[model[0] for model in estimators]\n",
    "\n",
    "plot_estimators(pipes=pipes, estimators=estimator_names, \n",
    "                n_splits=5, data=train_df.drop(columns='ups'), \n",
    "                target=train_df.ups, metrics=_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321b1eb-5aa9-4926-8f12-5bc9f1879cbc",
   "metadata": {},
   "source": [
    "For the rest of the modelling, we will only keep the Lightboost part. So let's fine tune the model by using a Bayesian Search Cross validation from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11e7ee7b-35c5-4d9e-b0cf-0430c2c3614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   58.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   54.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing data_prep, total=   4.4s\n",
      "[Pipeline] .............. (step 2 of 2) Processing lgbm, total=  10.5s\n",
      "best score: -8.25260641613107\n",
      "best params: OrderedDict([('lgbm__feature_fraction', 0.8195185734374574), ('lgbm__learning_rate', 0.06674786833218642), ('lgbm__max_depth', 10), ('lgbm__min_child_weight', 0.20874185467175646), ('lgbm__min_split_gain', 0.22636707243148557), ('lgbm__n_estimators', 233), ('lgbm__num_leaves', 99), ('lgbm__subsample', 0.969699780950025)])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dade3b777ab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m               'feature_fraction': Real(0.7, 0.9)}\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m lgbcv, results = tune_param(model='lgbm',\n\u001b[0m\u001b[1;32m     11\u001b[0m                             \u001b[0mpipes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                             \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-888664772010>\u001b[0m in \u001b[0;36mtune_param\u001b[0;34m(model, pipes, param_grid, refit, data, target, cv, n_iter)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best score: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best params: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mxgbcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "param_grid = {'num_leaves': Integer(20, 100),\n",
    "              'learning_rate': Real(1e-3, .3),\n",
    "              'n_estimators': Integer(100, 1000),\n",
    "              'subsample': Real(0.7, 1),\n",
    "              'max_depth': Integer(3, 15),\n",
    "              'min_child_weight': Real(0.01, 0.5),\n",
    "              'min_split_gain': Real(0, 0.3),\n",
    "              'feature_fraction': Real(0.7, 0.9)}\n",
    "\n",
    "lgbcv, results = tune_param(model='lgbm',\n",
    "                            pipes=pipes,\n",
    "                            param_grid=param_grid,\n",
    "                            refit=True,\n",
    "                            data = train_df.drop(columns='ups'),\n",
    "                            target=train_df.ups,\n",
    "                            cv=5,\n",
    "                            n_iter=20)\n",
    "best_params = lgbcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f04883f-8386-4b10-933f-09a53f461073",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params =  {'lgbm__feature_fraction': 0.8195185734374574, \n",
    "                'lgbm__learning_rate': 0.06674786833218642, \n",
    "                'lgbm__max_depth': 10, \n",
    "                'lgbm__min_child_weight': 0.20874185467175646,\n",
    "                'lgbm__min_split_gain': 0.22636707243148557, \n",
    "                'lgbm__n_estimators': 233,\n",
    "                'lgbm__num_leaves': 99,\n",
    "                'lgbm__subsample': 0.969699780950025}\n",
    "best_params = {k[6:]: v for k,v in best_params.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d37f70-c2fd-483b-b470-bab17ae900d0",
   "metadata": {},
   "source": [
    "We will now use the best parameters to train our model and make predictions.\n",
    "We will encode our categorical variables using OneHotEncoder, since the number of columns will be high we will sparse our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb60174-a717-4e7f-bc4c-71d3c2ba116c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding...\n",
      "Encoding done...\n",
      "sparsing...\n",
      "sparsing done...\n"
     ]
    }
   ],
   "source": [
    "Xx, yy = train_df.drop(columns=drop_cols), train_df.ups.values\n",
    "XX_t = test_df.drop(columns=drop_cols)\n",
    "\n",
    "\n",
    "print('Encoding...')\n",
    "for c in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(Xx[c])\n",
    "    Xx[c] = le.transform(Xx[c])\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train_df[categorical_cols])\n",
    "X_cat = enc.transform(Xx[categorical_cols])\n",
    "X_t_cat = enc.transform(XX_t[categorical_cols])\n",
    "print('Encoding done...')\n",
    "\n",
    "print('sparsing...')\n",
    "train_list = [Xx.drop(columns=categorical_cols).values,X_cat,]\n",
    "X = ssp.hstack(train_list).tocsr()\n",
    "test_list = [XX_t.drop(columns=categorical_cols).values,X_t_cat,]\n",
    "X_test = ssp.hstack(test_list).tocsr()\n",
    "print('sparsing done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aacff2-ed45-4573-a4d6-af0e51ce46ec",
   "metadata": {},
   "source": [
    "The last part is basically running a cross-validation on the train dataset and for each fold predicting our test set, and then taking the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24e8e3b6-e1eb-438d-a153-de7fdaf5759b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold number 1 / 5 on iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/othmane/anaconda3/envs/tf-38/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6892\n",
      "[LightGBM] [Info] Number of data points in the train set: 2574809, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[233]\tvalid_0's l1: 7.84827\n",
      "mae 7.848274036041041\n",
      "Fold number 2 / 5 on iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/othmane/anaconda3/envs/tf-38/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6872\n",
      "[LightGBM] [Info] Number of data points in the train set: 2574809, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[233]\tvalid_0's l1: 7.8109\n",
      "mae 7.810901499525997\n",
      "Fold number 3 / 5 on iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/othmane/anaconda3/envs/tf-38/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6877\n",
      "[LightGBM] [Info] Number of data points in the train set: 2574810, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[233]\tvalid_0's l1: 7.69638\n",
      "mae 7.696382448178506\n",
      "Fold number 4 / 5 on iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/othmane/anaconda3/envs/tf-38/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6910\n",
      "[LightGBM] [Info] Number of data points in the train set: 2574810, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[233]\tvalid_0's l1: 7.5756\n",
      "mae 7.575602831129641\n",
      "Fold number 5 / 5 on iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/othmane/anaconda3/envs/tf-38/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6880\n",
      "[LightGBM] [Info] Number of data points in the train set: 2574810, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[233]\tvalid_0's l1: 7.67992\n",
      "mae 7.679917393921936\n",
      "Fold number 1 / 5 on iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/othmane/anaconda3/envs/tf-38/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6860\n",
      "[LightGBM] [Info] Number of data points in the train set: 2574809, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[233]\tvalid_0's l1: 7.85879\n",
      "mae 7.858789709242043\n",
      "Fold number 2 / 5 on iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/othmane/anaconda3/envs/tf-38/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6869\n",
      "[LightGBM] [Info] Number of data points in the train set: 2574809, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[233]\tvalid_0's l1: 7.84787\n",
      "mae 7.847873605087671\n",
      "Fold number 3 / 5 on iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/othmane/anaconda3/envs/tf-38/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6907\n",
      "[LightGBM] [Info] Number of data points in the train set: 2574810, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[233]\tvalid_0's l1: 7.69648\n",
      "mae 7.6964767900519035\n",
      "Fold number 4 / 5 on iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/othmane/anaconda3/envs/tf-38/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6894\n",
      "[LightGBM] [Info] Number of data points in the train set: 2574810, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[233]\tvalid_0's l1: 7.56561\n",
      "mae 7.565608808584874\n",
      "Fold number 5 / 5 on iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/othmane/anaconda3/envs/tf-38/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6903\n",
      "[LightGBM] [Info] Number of data points in the train set: 2574810, number of used features: 64\n",
      "[LightGBM] [Info] Start training from score 1.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[233]\tvalid_0's l1: 7.70697\n",
      "mae 7.706966346269478\n"
     ]
    }
   ],
   "source": [
    "NFOLDS = 5\n",
    "N_ITER = 2\n",
    "kfold = KFold(n_splits=NFOLDS, shuffle=True, random_state=420)\n",
    "num_boost_round = 10000\n",
    "\n",
    "final_cv_train = np.zeros(len(Xx))\n",
    "final_cv_pred = np.zeros(len(test_df))\n",
    "\n",
    "for s in range(N_ITER):\n",
    "    cv_train = np.zeros(len(Xx))\n",
    "    cv_pred = np.zeros(len(test_df))\n",
    "\n",
    "    best_params['seed'] = s\n",
    "\n",
    "    kf = kfold.split(train_df)\n",
    "\n",
    "    best_trees = []\n",
    "    fold_scores = []\n",
    "\n",
    "    for i, (train_fold, validate) in enumerate(kf):\n",
    "        print(f\"Fold number {i + 1} / {NFOLDS} on iteration {s + 1}\")\n",
    "        X_train, X_validate, label_train, label_validate = \\\n",
    "            X[train_fold, :], X[validate, :], yy[train_fold], yy[validate]\n",
    "        dtrain = lgbm.Dataset(X_train, label_train)\n",
    "        dvalid = lgbm.Dataset(X_validate, label_validate, reference=dtrain)\n",
    "        bst = lgbm.train(best_params, dtrain, num_boost_round, \n",
    "                         valid_sets=dvalid, verbose_eval=500,\n",
    "                         early_stopping_rounds=100)\n",
    "        best_trees.append(bst.best_iteration)\n",
    "        cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "        cv_train[validate] += bst.predict(X_validate)\n",
    "\n",
    "        score = mean_absolute_error(label_validate, cv_train[validate])\n",
    "        print(\"mae\", score)\n",
    "        fold_scores.append(score)\n",
    "        \n",
    "    cv_pred /= NFOLDS\n",
    "    final_cv_train += cv_train\n",
    "    final_cv_pred += cv_pred\n",
    "\n",
    "final_preds = final_cv_pred / N_ITER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d140dba-c73f-485e-9865-b782adfff32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = pd.DataFrame({'id': test_df.id, 'predicted': final_preds})\n",
    "subs.to_csv('subs_lgbm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aceb518-d948-4aad-b802-27ebcb1f9847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
